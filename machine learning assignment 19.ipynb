{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f253d9",
   "metadata": {},
   "source": [
    "1. A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2 and that the first set of random centroid is 15, 32, and that the second set is 12, 30. ?\n",
    "Using the k-means method, create two clusters for each set of centroid described above.\n",
    "For each set of centroid values, calculate the SSE.\n",
    "\n",
    "\n",
    "ans. To create two clusters for each set of centroid values using the k-means method, we can follow these steps:\n",
    "\n",
    "Assign each data point to the nearest centroid.\n",
    "Recalculate the centroid for each cluster as the mean of all the data points in that cluster.\n",
    "Repeat steps 1 and 2 until the centroids converge or a maximum number of iterations is reached.\n",
    "Using the first set of random centroids (15, 32) and the k-means method:\n",
    "\n",
    "Iteration 1:\n",
    "Cluster 1: [5, 10, 15, 20, 25]\n",
    "Cluster 2: [30, 35]\n",
    "\n",
    "New centroids: (15, 20) and (32.5, 32.5)\n",
    "\n",
    "Iteration 2:\n",
    "Cluster 1: [5, 10, 15, 20, 25]\n",
    "Cluster 2: [30, 35]\n",
    "\n",
    "New centroids: (15, 20) and (32.5, 32.5)\n",
    "\n",
    "The centroids have converged, so we can calculate the SSE:\n",
    "\n",
    "SSE = Sum of squared errors within each cluster\n",
    "= Sum of (each data point - centroid)^2 for each cluster\n",
    "= [(5-15)^2 + (10-15)^2 + (15-15)^2 + (20-15)^2 + (25-15)^2] + [(30-32.5)^2 + (35-32.5)^2]\n",
    "= 750 + 62.5\n",
    "= 812.5\n",
    "\n",
    "Using the second set of random centroids (12, 30) and the k-means method:\n",
    "\n",
    "Iteration 1:\n",
    "Cluster 1: [5, 10, 15, 20]\n",
    "Cluster 2: [25, 30, 35]\n",
    "\n",
    "New centroids: (12.5, 12.5) and (30, 30)\n",
    "\n",
    "Iteration 2:\n",
    "Cluster 1: [5, 10, 15, 20]\n",
    "Cluster 2: [25, 30, 35]\n",
    "\n",
    "New centroids: (12.5, 12.5) and (30, 30)\n",
    "\n",
    "The centroids have converged, so we can calculate the SSE:\n",
    "\n",
    "SSE = Sum of squared errors within each cluster\n",
    "= Sum of (each data point - centroid)^2 for each cluster\n",
    "= [(5-12.5)^2 + (10-12.5)^2 + (15-12.5)^2 + (20-12.5)^2] + [(25-30)^2 + (30-30)^2 + (35-30)^2]\n",
    "= 512.5 + 62.5\n",
    "= 575\n",
    "\n",
    "Therefore, the SSE for the first set of centroids (15, 32) is 812.5 and the SSE for the second set of centroids (12, 30) is 575.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151f05a0",
   "metadata": {},
   "source": [
    " 2. Describe how the Market Basket Research makes use of association analysis concepts. \n",
    "\n",
    "ans. Market Basket Analysis (MBA) makes use of association analysis concepts to uncover the relationships between items that customers tend to purchase together. Association analysis is a technique that examines the co-occurrence of items in transactions or data sets. In MBA, the transactions refer to the purchases made by customers, and the items refer to the products or services that customers buy.\n",
    "\n",
    "MBA uses association analysis to identify itemsets or groups of items that frequently occur together in transactions. The analysis calculates the support, confidence, and lift measures to evaluate the strength of the association between itemsets. Support measures the frequency of occurrence of an itemset, confidence measures the likelihood of purchasing an item given the presence of another item, and lift measures the degree of association between two items.\n",
    "\n",
    "Using these measures, MBA can identify the items that are most frequently purchased together and provide insights into customer behavior and preferences. This information can be used to improve product placement, optimize pricing strategies, and develop targeted marketing campaigns that appeal to specific customer segments.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241c7acd",
   "metadata": {},
   "source": [
    "3. Give an example of the Apriori algorithm for learning association rules ?\n",
    "\n",
    "ans. Suppose we have a dataset of customer transactions at a grocery store. Each transaction consists of a set of items purchased by the customer. We want to find frequent itemsets (sets of items that often appear together in transactions) and use them to generate association rules (rules that describe the relationships between itemsets).\n",
    "\n",
    "First, we set a minimum support threshold, say 0.2. This means that an itemset must appear in at least 20% of transactions to be considered frequent.\n",
    "We scan the dataset and find all individual items that meet the minimum support threshold. Let's say we find {bread}, {milk}, {eggs}, {cheese}, {yogurt}, and {apples}.\n",
    "We generate all possible pairs of these frequent items and calculate their support (how often they appear together in transactions). Let's say we find {bread, milk}, {bread, eggs}, {bread, cheese}, {milk, eggs}, {milk, cheese}, {eggs, cheese}, {milk, yogurt}, {milk, apples}, {eggs, yogurt}, and {cheese, yogurt}.\n",
    "We select the pairs with support greater than the minimum threshold and use them to generate candidate itemsets of length 3 by merging the selected pairs. Let's say we find {bread, milk, eggs}, {bread, milk, cheese}, {milk, eggs, cheese}, and {milk, yogurt, apples}.\n",
    "We calculate the support of these candidate itemsets and select the ones that meet the minimum threshold. Let's say we find {bread, milk, cheese} and {milk, eggs, cheese} to be frequent itemsets.\n",
    "We generate association rules from these frequent itemsets and calculate their confidence (the proportion of transactions containing the antecedent that also contain the consequent). Let's say we find {bread, milk} => {cheese} and {milk, eggs} => {cheese} to have high confidence.\n",
    "We select the rules that meet the minimum confidence threshold, say 0.8, to be strong association rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe007d05",
   "metadata": {},
   "source": [
    "4. In hierarchical clustering, how is the distance between clusters measured? Explain how this metricis used to decide when to end the iteration.\n",
    "\n",
    "ans. In hierarchical clustering, the distance between clusters is typically measured using a distance metric such as Euclidean distance, Manhattan distance, or cosine similarity. The distance metric used depends on the type of data being clustered and the research question being investigated.\n",
    "\n",
    "Once the distance between all pairs of data points has been calculated, the hierarchical clustering algorithm begins by assigning each data point to its own cluster. Then, in each iteration of the algorithm, it identifies the two closest clusters (i.e., those with the smallest distance between them) and merges them into a single cluster. This process continues until all data points are assigned to a single cluster.\n",
    "\n",
    "To decide when to end the iteration, the algorithm employs a stopping criterion. One common criterion is to specify the desired number of clusters in advance, and to stop the iteration once that number of clusters has been reached. Another approach is to use a dendrogram, which is a visual representation of the hierarchy of clusters created by the algorithm. The dendrogram displays the distance between clusters as the height of the branches linking them, with the closest clusters being joined first. The user can visually inspect the dendrogram to determine the optimal number of clusters by identifying the point at which the rate of change in the distance between clusters slows down, indicating that further clustering is no longer meaningful. This is often referred to as the elbow method.\n",
    "\n",
    "Overall, hierarchical clustering is a versatile and widely used method for identifying structure in complex data, and the choice of distance metric and stopping criterion is an important part of the clustering process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7982a137",
   "metadata": {},
   "source": [
    "5.  In the k-means algorithm, how do you recompute the cluster centroids ?\n",
    "\n",
    "ans. In the k-means algorithm, the cluster centroids are recomputed in each iteration based on the mean of all the data points assigned to that cluster. Specifically, the algorithm follows these steps to recompute the cluster centroids:\n",
    "\n",
    "Assign each data point to the nearest centroid. This creates k clusters.\n",
    "For each cluster, compute the mean of all the data points assigned to it. This gives the new centroid of the cluster.\n",
    "Use the new centroids to re-assign each data point to the nearest centroid.\n",
    "Repeat steps 2-3 until the centroids no longer change or a maximum number of iterations is reached.\n",
    "The mean calculation is used to find the center of mass of all the data points in the cluster, which serves as a representative point for the cluster. By updating the centroid in each iteration based on the mean of the data points assigned to it, the algorithm adjusts the position of the centroid to better capture the structure of the data.\n",
    "\n",
    "Once the new centroids are computed, the algorithm proceeds to re-assign each data point to the nearest centroid based on the updated cluster assignments. This process is repeated until the centroids no longer change, indicating that the algorithm has converged to a stable clustering solution.\n",
    "\n",
    "Overall, the re-computation of cluster centroids is a key step in the k-means algorithm, as it updates the position of the representative points for each cluster and guides the algorithm towards a better clustering solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd93a20",
   "metadata": {},
   "source": [
    " 6. At the start of the clustering exercise, discuss one method for determining the required number of clusters ?\n",
    " \n",
    " ans. Determining the optimal number of clusters is an important step in clustering analysis, as it can have a significant impact on the resulting clusters and their interpretability. While there is no one-size-fits-all method for determining the required number of clusters, there are several approaches that can be used, depending on the nature of the data and the research question.\n",
    "\n",
    "One method for determining the required number of clusters at the start of the clustering exercise is the elbow method. This method involves plotting the within-cluster sum of squares (WCSS) against the number of clusters, and identifying the point where the rate of change in WCSS slows down. The WCSS is a measure of the variation within each cluster, and it decreases as the number of clusters increases. The idea behind the elbow method is to identify the point on the plot where the additional reduction in WCSS becomes marginal or \"levels off\", indicating that adding more clusters does not lead to a significant improvement in the clustering solution.\n",
    "\n",
    "To apply the elbow method, one can start by selecting a range of possible cluster numbers and running the clustering algorithm for each number. For each cluster number, compute the WCSS and plot it against the number of clusters. Then, visually inspect the plot and identify the elbow point, which corresponds to the optimal number of clusters for the data.\n",
    "\n",
    "It is worth noting that the elbow method is not foolproof and may not always provide a clear indication of the optimal number of clusters. In some cases, the plot may not have a clear elbow point, or the optimal number of clusters may depend on the specific research question or domain knowledge. Therefore, it is important to consider other methods as well and to carefully evaluate the clustering results in light of the research question and data characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dfc7bf",
   "metadata": {},
   "source": [
    "7. Discuss the k-means algorithm's advantages and disadvantages ?\n",
    "\n",
    "The k-means algorithm is a popular unsupervised learning algorithm that is used to cluster data into groups based on their similarities. Here are the advantages and disadvantages of using the k-means algorithm:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Simplicity: The k-means algorithm is easy to understand and implement. It is also computationally efficient and can be applied to large datasets.\n",
    "\n",
    "Scalability: The k-means algorithm is scalable and can handle a large number of observations and variables.\n",
    "\n",
    "Flexibility: The k-means algorithm is flexible and can be used with a wide range of data types, including continuous, categorical, and binary data.\n",
    "\n",
    "Interpretable: The results of the k-means algorithm are easy to interpret, as the algorithm provides clear and concise cluster assignments.\n",
    "\n",
    "Convergence: The k-means algorithm always converges, meaning that it will eventually find a stable solution.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Sensitivity to initial conditions: The k-means algorithm is sensitive to the initial conditions, and the choice of initial centroids can affect the clustering results.\n",
    "\n",
    "Determining the optimal number of clusters: The number of clusters (k) in the k-means algorithm needs to be specified in advance. Determining the optimal number of clusters can be difficult and can lead to suboptimal results.\n",
    "\n",
    "Assumption of spherical clusters: The k-means algorithm assumes that the clusters are spherical and have equal variance. This assumption may not hold for all datasets.\n",
    "\n",
    "Outlier sensitivity: The k-means algorithm is sensitive to outliers, which can significantly affect the clustering results.\n",
    "\n",
    "Non-guaranteed global optima: K-means is prone to converging to local optima which means it might not give the best possible cluster solution possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c0839b",
   "metadata": {},
   "source": [
    "9.  During your study, you discovered seven findings, which are listed in the data points below. Using the K-means algorithm, you want to build three clusters from these observations. The clusters C1, C2, and C3 have the following findings after the first iteration ?\n",
    "C1: (2,2), (4,4), (6,6); C2: (2,2), (4,4), (6,6); C3: (2,2), (4,4),\n",
    "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,\n",
    "C3: (5,5) and (9,9)\n",
    "What would the cluster centroids be if you were to run a second iteration? What would this clustering's SSE be?\n",
    "\n",
    "\n",
    "ans. To calculate the cluster centroids and SSE after the second iteration, we need to reassign the data points to the clusters based on their distances to the current cluster centroids, recalculate the centroids of each cluster, and calculate the SSE.\n",
    "\n",
    "Based on the findings given, the initial clusters are:\n",
    "C1: (2,2), (4,4), (6,6)\n",
    "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4)\n",
    "C3: (5,5), (9,9)\n",
    "\n",
    "First iteration:\n",
    "We calculate the centroids for each cluster:\n",
    "C1 centroid: (4, 4)\n",
    "C2 centroid: (0, 4)\n",
    "C3 centroid: (7, 7)\n",
    "\n",
    "Then, we reassign the data points to the nearest cluster:\n",
    "C1: (2,2), (4,4), (6,6), (5,5)\n",
    "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (9,9)\n",
    "C3: (4,4)\n",
    "\n",
    "Now, we recalculate the centroids:\n",
    "C1 centroid: (4.25, 4)\n",
    "C2 centroid: (1.8, 4.4)\n",
    "C3 centroid: (4,4)\n",
    "\n",
    "Second iteration:\n",
    "We reassign the data points to the nearest cluster:\n",
    "C1: (2,2), (4,4), (6,6), (5,5), (4,4)\n",
    "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (5,5), (9,9)\n",
    "C3: (6,6)\n",
    "\n",
    "Now, we recalculate the centroids:\n",
    "C1 centroid: (4.2, 4.2)\n",
    "C2 centroid: (1.7, 4.2)\n",
    "C3 centroid: (6,6)\n",
    "\n",
    "The SSE is the sum of the squared distances between each data point and its cluster centroid. After the second iteration, the SSE is:\n",
    "SSE = (0.05^2 + 0.2^2 + 0.2^2 + 1.3^2 + 0.05^2 + 3.3^2 + 0.8^2 + 3.3^2 + 0.2^2 + 0.8^2 + 0.05^2) + (4^2 + 16^2 + 16^2 + 16^2 + 16^2 + 16^2 + 16^2 + 16^2 + 1^2 + 1^2) + (0.8^2)\n",
    "SSE = 281.45\n",
    "\n",
    "Therefore, after the second iteration, the cluster centroids are: C1 centroid: (4.2, 4.2), C2 centroid: (1.7, 4.2), C3 centroid: (6,6), and the SSE is 281.45.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b57d1ee",
   "metadata": {},
   "source": [
    "10. In a software project, the team is attempting to determine if software flaws discovered during testing are identical. Based on the text analytics of the defect details, they decided to build 5 clusters of related defects. Any new defect formed after the 5 clusters of defects have been identified must be listed as one of the forms identified by clustering. A simple diagram can be used to explain this process. Assume you have 20 defect data points that are clustered into 5 clusters and you used the k-means algorithm ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7800744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
